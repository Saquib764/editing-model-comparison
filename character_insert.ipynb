{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the latest diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`flash-attn` is not available or the version is too old. Please install `flash-attn>=2.6.3`.\n",
      "`sageattention` is not available or the version is too old. Please install `sageattention>=2.1.1`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import FluxKontextPipeline, QwenImageEditPipeline\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable any one\n",
    "\n",
    "mode = \"MANUAL\"      # Use your own instructions, and image\n",
    "# mode = \"AUTO\"        # Use instructions, and image from the folder\n",
    "\n",
    "output_dir = \"outputs/\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "  os.makedirs(output_dir)\n",
    "\n",
    "# For manual\n",
    "instructions = [\n",
    "  \"Add character to the image.\",\n",
    "]\n",
    "\n",
    "base_images = [\n",
    "  \"assets/scene_01.png\",\n",
    "]\n",
    "\n",
    "# For auto\n",
    "default_instruction = \"Add character to the image.\"\n",
    "\n",
    "# This folder has an image and a text file with instructions, with the same name\n",
    "base_src = \"example_data/\"\n",
    "\n",
    "if mode == \"AUTO\":\n",
    "  # load all images from the folder\n",
    "  base_images_name = [f for f in os.listdir(base_src) if f.endswith(('.png', '.jpg', '.jpeg', '.webp'))]\n",
    "  instructions = []\n",
    "  base_images = []\n",
    "  for base_image_name in base_images_name:\n",
    "    name = base_image_name.split(\".\")[0]\n",
    "    base_images.append(os.path.join(base_src, base_image_name))\n",
    "    _instruction = default_instruction\n",
    "    if os.path.exists(os.path.join(base_src, f\"{name}.txt\")):\n",
    "      with open(os.path.join(base_src, f\"{name}.txt\"), \"r\") as f:\n",
    "        _instruction = f.read()\n",
    "    instructions.append(_instruction)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: MANUAL\n",
      "Total images: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mode: {mode}\")\n",
    "print(f\"Total images: {len(base_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|                                                 | 0/7 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|                                                      | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████| 3/3 [00:00<00:00, 28.23it/s]\u001b[A\n",
      "Loading pipeline components...:  29%|███████████▋                             | 2/7 [00:00<00:00,  5.18it/s]You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loading pipeline components...:  86%|███████████████████████████████████▏     | 6/7 [00:00<00:00, 10.27it/s]\n",
      "Loading checkpoint shards:   0%|                                                      | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|███████████████████████                       | 1/2 [00:00<00:00,  6.15it/s]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████| 2/2 [00:00<00:00,  6.44it/s]\u001b[A\n",
      "Loading pipeline components...: 100%|█████████████████████████████████████████| 7/7 [00:01<00:00,  6.88it/s]\n"
     ]
    }
   ],
   "source": [
    "pipe = FluxKontextPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-Kontext-dev\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 14/14 [00:20<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "result_img = None\n",
    "model = \"flux\"\n",
    "i = 0\n",
    "for image_path, instruction in zip(base_images, instructions):\n",
    "    i += 1\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    width, height = image.size\n",
    "\n",
    "    seed = torch.Generator().manual_seed(42)\n",
    "\n",
    "    result_img = pipe(\n",
    "        prompt=instruction,\n",
    "        image=image,\n",
    "        num_inference_steps=14,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        generator=seed,\n",
    "    ).images[0]\n",
    "\n",
    "    result_img.save(f\"{output_dir}/{i:03d}_{model}.png\")\n",
    "\n",
    "print(\"Last image\")\n",
    "result_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = QwenImageEditPipeline.from_pretrained(\n",
    "    \"Qwen/Qwen-Image-Edit\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "print(\"pipeline loaded\")\n",
    "pipe.to(\"cuda\")\n",
    "pipe.set_progress_bar_config(disable=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"qwen\"\n",
    "i = 0\n",
    "\n",
    "for image_path, instruction in zip(base_images, instructions):\n",
    "    i += 1\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    seed = torch.Generator().manual_seed(42)\n",
    "\n",
    "    output_image = pipe(\n",
    "        image=image,\n",
    "        prompt=instruction,\n",
    "        generator=seed,\n",
    "        true_cfg_scale=4.0,\n",
    "        negative_prompt=\" \",\n",
    "        num_inference_steps=14,\n",
    "    ).images[0]\n",
    "\n",
    "    output_image.save(f\"{output_dir}/{i:03d}_{model}.png\")\n",
    "\n",
    "print(\"Last image\")\n",
    "output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zustpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
